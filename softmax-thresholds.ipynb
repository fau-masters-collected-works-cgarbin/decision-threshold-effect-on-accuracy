{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "1f1c1f8ebd410d0498d06e54258f8fd06f8ecdea47bbd9105d5c0a6dba584e46"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Effect of using thresholds with softmax\n",
    "\n",
    "This notebook shows the effect of choosing thresholds for the softmax layer in classification problems.\n",
    "\n",
    "The usual approach is to choose the maximum of the predicted probabilities:\n",
    "\n",
    "... code here <<<<<<<<\n",
    "\n",
    "However, this gloses over the fact that sometimes two or more classes have very close predicted probabilities.\n",
    "\n",
    "Using the MNIST dataset, we will investigate the effect of settings thresholds to make predictions, instead of simply the maximum probability.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Tensorflow MNIST example](https://www.tensorflow.org/datasets/keras_example)\n",
    "- [Multi-label vs. Multi-class Classification: Sigmoid vs. Softmax](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)\n",
    "- [Interpreting logits: sigmoid vs softmax](https://web.stanford.edu/~nanbhas/blog/sigmoid-softmax.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Load MNIST and create the training/test pipelines\n",
    "\n",
    "The code in this section is from the [TensorFlow MNIST example](https://www.tensorflow.org/datasets/keras_example)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "source": [
    "Load MNIST."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/cgarbin/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.99 file/s]\n",
      "\u001b[1mDataset mnist downloaded and prepared to /Users/cgarbin/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "source": [
    "Build training pipeline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "source": [
    "Build test pipeline."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "source": [
    "## Train a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")"
   ]
  },
  {
   "source": [
    "Use early stopping to get a decent model without investing too much time in training. The main purspose of this notebook is not to get high accuracy. It's to demonstrate the effect of softmax thresholds."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0010 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.1310 - val_sparse_categorical_accuracy: 0.9771\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 2.0192e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1264 - val_sparse_categorical_accuracy: 0.9787\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1655e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1260 - val_sparse_categorical_accuracy: 0.9791\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 9.5013e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 8.1477e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1260 - val_sparse_categorical_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 7.1974e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1260 - val_sparse_categorical_accuracy: 0.9791\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 6.3866e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1264 - val_sparse_categorical_accuracy: 0.9792\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 5.6083e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1265 - val_sparse_categorical_accuracy: 0.9794\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 5.0067e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1268 - val_sparse_categorical_accuracy: 0.9791\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 4.5188e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1276 - val_sparse_categorical_accuracy: 0.9796\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa9049aa6d0>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=2)\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=10,\n",
    "    validation_data=ds_test,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}